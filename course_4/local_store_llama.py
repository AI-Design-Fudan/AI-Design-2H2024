# from transformers import AutoModelForCausalLM, AutoTokenizer

# model_id = "meta-llama/Llama-3.2-1B"
# tokenizer_id = "meta-llama/Llama-3.2-1B"

# # 加载模型和tokenizer
# model = AutoModelForCausalLM.from_pretrained(model_id)
# tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)

# # 保存到本地
# model.save_pretrained('./final_model')


# print("模型和tokenizer已成功保存到 './final_model'。")
